{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00ba5eed-2d52-483e-a1a1-48ad45be7bfb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Fine-Tuning with Ray AIR and DeepSpeed on Databricks\n",
    "\n",
    "In this example, we will showcase how to use the Ray AIR for **pythia-12b**. These causal language model trained on the Pile dataset(825 GB). This particular model has 12 billion parameters. For more information on pythia-12b click [here](https://huggingface.co/EleutherAI/pythia-12b).\n",
    "\n",
    "We will use Ray AIR (with the 🤗 Transformers integration) and a pretrained model from Hugging Face hub. Note that you can easily adapt this example to use other similar models.\n",
    "\n",
    "This example focuses more on the performance and distributed computing aspects of Ray AIR. \n",
    "\n",
    "It is highly recommended to read [Ray AIR Key Concepts](https://raw.githubusercontent.com/ray-project/ray/master/doc/source/ray-air/examples/air-key-concepts) and [Ray Data Key Concepts](https://raw.githubusercontent.com/ray-project/ray/master/doc/source/ray-air/examples/data_key_concepts) before starting this example.\n",
    "\n",
    "```{note}\n",
    "In order to run this example, make sure your Ray cluster has access to at least 8 GPU's with 24 or more GBs of memory. The amount of memory needed will depend on the model. This notebook has been  tested with 4 g5.24xlarge workers and g4dn.8xlarge head node.\n",
    "```\n",
    "Benefits:\n",
    "\n",
    "- No command-line trigger / and provides real-time updates\n",
    "- Better UI to understand JOB performance and adjust the batch-size and performance configd\n",
    "- Ray data API support data in parquet,csv and hf format\n",
    "- Has MLFlow integration to track Experiments\n",
    "\n",
    "In this notebook, we will:\n",
    "1. [Add Dependencies to run deepspeed](#Deepspeed)\n",
    "2. [Set up Ray](#setup)\n",
    "3. [Load the dataset](#load)\n",
    "4. [Preprocess the dataset with Ray AIR](#preprocess)\n",
    "5. [Run the training with Ray AIR](#train)\n",
    "6. [Generate text from prompt with Ray AIR](#predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3df7e858-e866-4161-871f-c56f5506c405",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Load Dependencies for Deepspeed <a name=\"deepspeed\"></a>\n",
    "Uncomment and run the following line in order to create an init script which loads the dependencies required for Deepspeed (this notebook is being tested with `transformers==4.26.0`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bde920f3-433c-401d-adfd-8202b220b1e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# kernel_gateway_init = \"\"\"\n",
    "# #!/bin/bash\n",
    "\n",
    "# wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/libcusparse-dev-11-3_11.5.0.58-1_amd64.deb -O /tmp/libcusparse-dev-11-3_11.5.0.58-1_amd64.deb && \\\n",
    "# wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/libcublas-dev-11-3_11.5.1.109-1_amd64.deb -O /tmp/libcublas-dev-11-3_11.5.1.109-1_amd64.deb && \\\n",
    "# wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/libcusolver-dev-11-3_11.1.2.109-1_amd64.deb -O /tmp/libcusolver-dev-11-3_11.1.2.109-1_amd64.deb && \\\n",
    "# wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/libcurand-dev-11-3_10.2.4.109-1_amd64.deb -O /tmp/libcurand-dev-11-3_10.2.4.109-1_amd64.deb && \\\n",
    "# dpkg -i /tmp/libcusparse-dev-11-3_11.5.0.58-1_amd64.deb && \\\n",
    "# dpkg -i /tmp/libcublas-dev-11-3_11.5.1.109-1_amd64.deb && \\\n",
    "# dpkg -i /tmp/libcusolver-dev-11-3_11.1.2.109-1_amd64.deb && \\\n",
    "# dpkg -i /tmp/libcurand-dev-11-3_10.2.4.109-1_amd64.deb\n",
    "# \"\"\" \n",
    "# # Change ‘username’ to your Databricks username in DBFS\n",
    "# # Example: username = “stephen.offer@databricks.com”\n",
    "# username = \"puneet.jain@databricks.com\"\n",
    "# dbutils.fs.put(\"dbfs:/Users/{0}/init/ray.sh\".format(username), kernel_gateway_init, True)\n",
    "# \"dbfs:/Users/{0}/init/ray.sh\".format(username)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c83bc06-bd72-44e4-8bc1-dc228360b85b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Set up Ray <a name=\"setup\"></a>\n",
    "\n",
    "First, Let us start a ray cluster based on the cluster configuration. we need to specify the number of cores and gpus available per worker to **setup_ray_cluster** to create the correct multi-node setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c04cee40-2348-4d31-bbfd-8edfa757dd54",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting accelerate<0.19.0,>=0.18.0\n  Downloading accelerate-0.18.0-py3-none-any.whl (215 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 215.3/215.3 kB 3.7 MB/s eta 0:00:00\nRequirement already satisfied: click<9,>=8.0.4 in /databricks/python3/lib/python3.10/site-packages (from -r requirement.txt (line 2)) (8.0.4)\nRequirement already satisfied: datasets<3,>=2.10.0 in /databricks/python3/lib/python3.10/site-packages (from -r requirement.txt (line 3)) (2.10.0)\nCollecting deepspeed<0.9.0,>=0.8.3\n  Downloading deepspeed-0.8.3.tar.gz (765 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 765.4/765.4 kB 8.8 MB/s eta 0:00:00\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nCollecting transformers[torch]<5,>=4.28.1\n  Downloading transformers-4.30.1-py3-none-any.whl (7.2 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.2/7.2 MB 26.1 MB/s eta 0:00:00\nCollecting langchain>=0.0.139\n  Downloading langchain-0.0.195-py3-none-any.whl (1.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 47.0 MB/s eta 0:00:00\nCollecting awscli\n  Downloading awscli-1.27.151-py3-none-any.whl (4.1 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.1/4.1 MB 58.4 MB/s eta 0:00:00\nRequirement already satisfied: fsspec in /databricks/python3/lib/python3.10/site-packages (from -r requirement.txt (line 8)) (2022.7.1)\nRequirement already satisfied: torch<2,>=1.13.1 in /databricks/python3/lib/python3.10/site-packages (from -r requirement.txt (line 9)) (1.13.1+cu117)\nCollecting ray[default]==2.5.0\n  Using cached ray-2.5.0-cp310-cp310-manylinux2014_x86_64.whl (56.2 MB)\nRequirement already satisfied: numpy>=1.19.3 in /databricks/python3/lib/python3.10/site-packages (from ray[default]==2.5.0->-r requirement.txt (line 10)) (1.21.5)\nRequirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /databricks/python3/lib/python3.10/site-packages (from ray[default]==2.5.0->-r requirement.txt (line 10)) (3.19.4)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.10/site-packages (from ray[default]==2.5.0->-r requirement.txt (line 10)) (2.28.1)\nRequirement already satisfied: jsonschema in /databricks/python3/lib/python3.10/site-packages (from ray[default]==2.5.0->-r requirement.txt (line 10)) (4.16.0)\nRequirement already satisfied: pyyaml in /databricks/python3/lib/python3.10/site-packages (from ray[default]==2.5.0->-r requirement.txt (line 10)) (6.0)\nRequirement already satisfied: grpcio<=1.51.3,>=1.42.0 in /databricks/python3/lib/python3.10/site-packages (from ray[default]==2.5.0->-r requirement.txt (line 10)) (1.48.1)\nCollecting msgpack<2.0.0,>=1.0.0\n  Using cached msgpack-1.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (316 kB)\nRequirement already satisfied: filelock in /databricks/python3/lib/python3.10/site-packages (from ray[default]==2.5.0->-r requirement.txt (line 10)) (3.6.0)\nRequirement already satisfied: attrs in /databricks/python3/lib/python3.10/site-packages (from ray[default]==2.5.0->-r requirement.txt (line 10)) (21.4.0)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.10/site-packages (from ray[default]==2.5.0->-r requirement.txt (line 10)) (21.3)\nRequirement already satisfied: aiosignal in /databricks/python3/lib/python3.10/site-packages (from ray[default]==2.5.0->-r requirement.txt (line 10)) (1.3.1)\nRequirement already satisfied: frozenlist in /databricks/python3/lib/python3.10/site-packages (from ray[default]==2.5.0->-r requirement.txt (line 10)) (1.3.3)\nCollecting gpustat>=1.0.0\n  Using cached gpustat-1.1-py3-none-any.whl\nRequirement already satisfied: aiohttp>=3.7 in /databricks/python3/lib/python3.10/site-packages (from ray[default]==2.5.0->-r requirement.txt (line 10)) (3.8.4)\nCollecting py-spy>=0.2.0\n  Using cached py_spy-0.3.14-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (3.0 MB)\nCollecting colorful\n  Using cached colorful-0.5.5-py2.py3-none-any.whl (201 kB)\nCollecting aiohttp-cors\n  Using cached aiohttp_cors-0.7.0-py3-none-any.whl (27 kB)\nCollecting prometheus-client>=0.7.1\n  Using cached prometheus_client-0.17.0-py3-none-any.whl (60 kB)\nRequirement already satisfied: virtualenv<20.21.1,>=20.0.24 in /usr/local/lib/python3.10/dist-packages (from ray[default]==2.5.0->-r requirement.txt (line 10)) (20.16.3)\nRequirement already satisfied: pydantic in /databricks/python3/lib/python3.10/site-packages (from ray[default]==2.5.0->-r requirement.txt (line 10)) (1.10.7)\nRequirement already satisfied: smart-open in /databricks/python3/lib/python3.10/site-packages (from ray[default]==2.5.0->-r requirement.txt (line 10)) (5.2.1)\nCollecting opencensus\n  Using cached opencensus-0.11.2-py2.py3-none-any.whl (128 kB)\nCollecting lz4\n  Using cached lz4-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\nCollecting dm-tree\n  Using cached dm_tree-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (152 kB)\nCollecting tensorboardX>=1.9\n  Using cached tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\nRequirement already satisfied: pyarrow>=6.0.1 in /databricks/python3/lib/python3.10/site-packages (from ray[default]==2.5.0->-r requirement.txt (line 10)) (7.0.0)\nRequirement already satisfied: pandas in /databricks/python3/lib/python3.10/site-packages (from ray[default]==2.5.0->-r requirement.txt (line 10)) (1.4.4)\nRequirement already satisfied: scipy in /databricks/python3/lib/python3.10/site-packages (from ray[default]==2.5.0->-r requirement.txt (line 10)) (1.9.1)\nCollecting scikit-image\n  Using cached scikit_image-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.8 MB)\nCollecting gymnasium==0.26.3\n  Using cached Gymnasium-0.26.3-py3-none-any.whl (836 kB)\nCollecting rich\n  Using cached rich-13.4.1-py3-none-any.whl (239 kB)\nRequirement already satisfied: typer in /databricks/python3/lib/python3.10/site-packages (from ray[default]==2.5.0->-r requirement.txt (line 10)) (0.7.0)\nRequirement already satisfied: cloudpickle>=1.2.0 in /databricks/python3/lib/python3.10/site-packages (from gymnasium==0.26.3->ray[default]==2.5.0->-r requirement.txt (line 10)) (2.0.0)\nCollecting gymnasium-notices>=0.0.1\n  Using cached gymnasium_notices-0.0.1-py3-none-any.whl (2.8 kB)\nRequirement already satisfied: psutil in /databricks/python3/lib/python3.10/site-packages (from accelerate<0.19.0,>=0.18.0->-r requirement.txt (line 1)) (5.9.0)\nRequirement already satisfied: xxhash in /databricks/python3/lib/python3.10/site-packages (from datasets<3,>=2.10.0->-r requirement.txt (line 3)) (3.2.0)\nRequirement already satisfied: dill<0.3.7,>=0.3.0 in /databricks/python3/lib/python3.10/site-packages (from datasets<3,>=2.10.0->-r requirement.txt (line 3)) (0.3.4)\nRequirement already satisfied: responses<0.19 in /databricks/python3/lib/python3.10/site-packages (from datasets<3,>=2.10.0->-r requirement.txt (line 3)) (0.18.0)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /databricks/python3/lib/python3.10/site-packages (from datasets<3,>=2.10.0->-r requirement.txt (line 3)) (0.13.3)\nRequirement already satisfied: tqdm>=4.62.1 in /databricks/python3/lib/python3.10/site-packages (from datasets<3,>=2.10.0->-r requirement.txt (line 3)) (4.64.1)\nRequirement already satisfied: multiprocess in /databricks/python3/lib/python3.10/site-packages (from datasets<3,>=2.10.0->-r requirement.txt (line 3)) (0.70.12.2)\nCollecting hjson\n  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.0/54.0 kB 10.1 MB/s eta 0:00:00\nCollecting ninja\n  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 146.0/146.0 kB 23.3 MB/s eta 0:00:00\nCollecting py-cpuinfo\n  Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /databricks/python3/lib/python3.10/site-packages (from transformers[torch]<5,>=4.28.1->-r requirement.txt (line 5)) (0.13.2)\nCollecting safetensors>=0.3.1\n  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 70.9 MB/s eta 0:00:00\nCollecting huggingface-hub<1.0.0,>=0.2.0\n  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 236.8/236.8 kB 31.5 MB/s eta 0:00:00\nRequirement already satisfied: regex!=2019.12.17 in /databricks/python3/lib/python3.10/site-packages (from transformers[torch]<5,>=4.28.1->-r requirement.txt (line 5)) (2022.7.9)\nCollecting transformers[torch]<5,>=4.28.1\n  Downloading transformers-4.30.0-py3-none-any.whl (7.2 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.2/7.2 MB 82.5 MB/s eta 0:00:00\n  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.1/7.1 MB 97.4 MB/s eta 0:00:00\n  Downloading transformers-4.29.1-py3-none-any.whl (7.1 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.1/7.1 MB 109.6 MB/s eta 0:00:00\n  Downloading transformers-4.29.0-py3-none-any.whl (7.1 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.1/7.1 MB 111.5 MB/s eta 0:00:00\n  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.0/7.0 MB 96.9 MB/s eta 0:00:00\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /databricks/python3/lib/python3.10/site-packages (from langchain>=0.0.139->-r requirement.txt (line 6)) (4.0.2)\nCollecting SQLAlchemy<3,>=1.4\n  Downloading SQLAlchemy-2.0.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.7/2.7 MB 103.6 MB/s eta 0:00:00\nCollecting numexpr<3.0.0,>=2.8.4\n  Downloading numexpr-2.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (381 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 381.4/381.4 kB 50.3 MB/s eta 0:00:00\nCollecting tenacity<9.0.0,>=8.1.0\n  Downloading tenacity-8.2.2-py3-none-any.whl (24 kB)\nCollecting openapi-schema-pydantic<2.0,>=1.2\n  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 90.0/90.0 kB 16.7 MB/s eta 0:00:00\nCollecting dataclasses-json<0.6.0,>=0.5.7\n  Downloading dataclasses_json-0.5.7-py3-none-any.whl (25 kB)\nCollecting langchainplus-sdk>=0.0.7\n  Downloading langchainplus_sdk-0.0.8-py3-none-any.whl (22 kB)\nCollecting botocore==1.29.151\n  Downloading botocore-1.29.151-py3-none-any.whl (10.9 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.9/10.9 MB 96.2 MB/s eta 0:00:00\nRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /databricks/python3/lib/python3.10/site-packages (from awscli->-r requirement.txt (line 7)) (0.6.0)\nCollecting pyyaml\n  Downloading PyYAML-5.4.1.tar.gz (175 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 175.1/175.1 kB 29.9 MB/s eta 0:00:00\n  Installing build dependencies: started\n  Installing build dependencies: finished with status 'done'\n  Getting requirements to build wheel: started\n  Getting requirements to build wheel: finished with status 'done'\n  Preparing metadata (pyproject.toml): started\n  Preparing metadata (pyproject.toml): finished with status 'done'\nCollecting colorama<0.4.5,>=0.2.5\n  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\nCollecting docutils<0.17,>=0.10\n  Downloading docutils-0.16-py2.py3-none-any.whl (548 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 548.2/548.2 kB 63.7 MB/s eta 0:00:00\nCollecting rsa<4.8,>=3.1.2\n  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /databricks/python3/lib/python3.10/site-packages (from botocore==1.29.151->awscli->-r requirement.txt (line 7)) (2.8.2)\nRequirement already satisfied: urllib3<1.27,>=1.25.4 in /databricks/python3/lib/python3.10/site-packages (from botocore==1.29.151->awscli->-r requirement.txt (line 7)) (1.26.11)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /databricks/python3/lib/python3.10/site-packages (from botocore==1.29.151->awscli->-r requirement.txt (line 7)) (0.10.0)\nRequirement already satisfied: typing-extensions in /databricks/python3/lib/python3.10/site-packages (from torch<2,>=1.13.1->-r requirement.txt (line 9)) (4.3.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /databricks/python3/lib/python3.10/site-packages (from aiohttp>=3.7->ray[default]==2.5.0->-r requirement.txt (line 10)) (2.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /databricks/python3/lib/python3.10/site-packages (from aiohttp>=3.7->ray[default]==2.5.0->-r requirement.txt (line 10)) (1.8.2)\nRequirement already satisfied: multidict<7.0,>=4.5 in /databricks/python3/lib/python3.10/site-packages (from aiohttp>=3.7->ray[default]==2.5.0->-r requirement.txt (line 10)) (6.0.4)\nCollecting marshmallow-enum<2.0.0,>=1.5.1\n  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\nCollecting marshmallow<4.0.0,>=3.3.0\n  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 49.1/49.1 kB 10.4 MB/s eta 0:00:00\nCollecting typing-inspect>=0.4.0\n  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\nCollecting nvidia-ml-py>=11.450.129\n  Using cached nvidia_ml_py-11.525.112-py3-none-any.whl (35 kB)\nCollecting blessed>=1.17.1\n  Using cached blessed-1.20.0-py2.py3-none-any.whl (58 kB)\nRequirement already satisfied: six>=1.5.2 in /usr/lib/python3/dist-packages (from grpcio<=1.51.3,>=1.42.0->ray[default]==2.5.0->-r requirement.txt (line 10)) (1.16.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /databricks/python3/lib/python3.10/site-packages (from packaging->ray[default]==2.5.0->-r requirement.txt (line 10)) (3.0.9)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.10/site-packages (from requests->ray[default]==2.5.0->-r requirement.txt (line 10)) (2022.9.14)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.10/site-packages (from requests->ray[default]==2.5.0->-r requirement.txt (line 10)) (3.3)\nRequirement already satisfied: pyasn1>=0.1.3 in /databricks/python3/lib/python3.10/site-packages (from rsa<4.8,>=3.1.2->awscli->-r requirement.txt (line 7)) (0.4.8)\nCollecting greenlet!=0.4.17\n  Downloading greenlet-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (613 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 613.7/613.7 kB 61.4 MB/s eta 0:00:00\nRequirement already satisfied: platformdirs<3,>=2.4 in /databricks/python3/lib/python3.10/site-packages (from virtualenv<20.21.1,>=20.0.24->ray[default]==2.5.0->-r requirement.txt (line 10)) (2.5.2)\nRequirement already satisfied: distlib<1,>=0.3.5 in /usr/local/lib/python3.10/dist-packages (from virtualenv<20.21.1,>=20.0.24->ray[default]==2.5.0->-r requirement.txt (line 10)) (0.3.6)\nRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /databricks/python3/lib/python3.10/site-packages (from jsonschema->ray[default]==2.5.0->-r requirement.txt (line 10)) (0.18.0)\nCollecting opencensus-context>=0.1.3\n  Using cached opencensus_context-0.1.3-py2.py3-none-any.whl (5.1 kB)\nCollecting google-api-core<3.0.0,>=1.0.0\n  Using cached google_api_core-2.11.0-py3-none-any.whl (120 kB)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.10/site-packages (from pandas->ray[default]==2.5.0->-r requirement.txt (line 10)) (2022.1)\nCollecting markdown-it-py<3.0.0,>=2.2.0\n  Using cached markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\nCollecting pygments<3.0.0,>=2.13.0\n  Using cached Pygments-2.15.1-py3-none-any.whl (1.1 MB)\nRequirement already satisfied: PyWavelets>=1.1.1 in /databricks/python3/lib/python3.10/site-packages (from scikit-image->ray[default]==2.5.0->-r requirement.txt (line 10)) (1.3.0)\nCollecting lazy_loader>=0.2\n  Using cached lazy_loader-0.2-py3-none-any.whl (8.6 kB)\nCollecting imageio>=2.27\n  Using cached imageio-2.31.0-py3-none-any.whl (313 kB)\nCollecting tifffile>=2022.8.12\n  Using cached tifffile-2023.4.12-py3-none-any.whl (219 kB)\nRequirement already satisfied: networkx>=2.8 in /databricks/python3/lib/python3.10/site-packages (from scikit-image->ray[default]==2.5.0->-r requirement.txt (line 10)) (2.8.4)\nRequirement already satisfied: pillow>=9.0.1 in /databricks/python3/lib/python3.10/site-packages (from scikit-image->ray[default]==2.5.0->-r requirement.txt (line 10)) (9.2.0)\nRequirement already satisfied: wcwidth>=0.1.4 in /databricks/python3/lib/python3.10/site-packages (from blessed>=1.17.1->gpustat>=1.0.0->ray[default]==2.5.0->-r requirement.txt (line 10)) (0.2.5)\nCollecting google-auth<3.0dev,>=2.14.1\n  Using cached google_auth-2.19.1-py2.py3-none-any.whl (181 kB)\nCollecting protobuf!=3.19.5,>=3.15.3\n  Using cached protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\nRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /databricks/python3/lib/python3.10/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]==2.5.0->-r requirement.txt (line 10)) (1.56.4)\nCollecting mdurl~=0.1\n  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /databricks/python3/lib/python3.10/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain>=0.0.139->-r requirement.txt (line 6)) (0.4.3)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.10/site-packages (from google-auth<3.0dev,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]==2.5.0->-r requirement.txt (line 10)) (0.2.8)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /databricks/python3/lib/python3.10/site-packages (from google-auth<3.0dev,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]==2.5.0->-r requirement.txt (line 10)) (4.2.4)\nBuilding wheels for collected packages: deepspeed, pyyaml\n  Building wheel for deepspeed (setup.py): started\n  Building wheel for deepspeed (setup.py): finished with status 'done'\n  Created wheel for deepspeed: filename=deepspeed-0.8.3-py3-none-any.whl size=776405 sha256=435f40894d99221bf178d0228271eacef8a2dd7207500be28a4e51cf1ed9aafd\n  Stored in directory: /root/.cache/pip/wheels/1c/f2/89/18040d789487945acbfd6a90142fd0cb4f34748513a5d65f0b\n  Building wheel for pyyaml (pyproject.toml): started\n  Building wheel for pyyaml (pyproject.toml): finished with status 'done'\n  Created wheel for pyyaml: filename=PyYAML-5.4.1-cp310-cp310-linux_x86_64.whl size=45656 sha256=ad2636279af63c00f81ff091105264d3745bbbfbff24012fdb9867dc7f138f6c\n  Stored in directory: /root/.cache/pip/wheels/c7/0d/22/696ee92245ad710f506eee79bb05c740d8abccd3ecdb778683\nSuccessfully built deepspeed pyyaml\nInstalling collected packages: py-spy, py-cpuinfo, opencensus-context, nvidia-ml-py, ninja, msgpack, hjson, gymnasium-notices, dm-tree, colorful, typing-inspect, tifffile, tenacity, rsa, pyyaml, pygments, protobuf, prometheus-client, numexpr, mdurl, lz4, lazy_loader, imageio, gymnasium, greenlet, docutils, colorama, blessed, tensorboardX, SQLAlchemy, scikit-image, ray, openapi-schema-pydantic, marshmallow, markdown-it-py, langchainplus-sdk, gpustat, google-auth, deepspeed, botocore, accelerate, transformers, rich, marshmallow-enum, google-api-core, aiohttp-cors, opencensus, dataclasses-json, awscli, langchain\n  Attempting uninstall: tenacity\n    Found existing installation: tenacity 8.0.1\n    Not uninstalling tenacity at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-e87ab28a-edc0-4502-b88f-c5254be3f0f2\n    Can't uninstall 'tenacity'. No files were found to uninstall.\n  Attempting uninstall: rsa\n    Found existing installation: rsa 4.9\n    Not uninstalling rsa at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-e87ab28a-edc0-4502-b88f-c5254be3f0f2\n    Can't uninstall 'rsa'. No files were found to uninstall.\n  Attempting uninstall: pyyaml\n    Found existing installation: PyYAML 6.0\n    Not uninstalling pyyaml at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-e87ab28a-edc0-4502-b88f-c5254be3f0f2\n    Can't uninstall 'PyYAML'. No files were found to uninstall.\n  Attempting uninstall: pygments\n    Found existing installation: Pygments 2.11.2\n    Not uninstalling pygments at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-e87ab28a-edc0-4502-b88f-c5254be3f0f2\n    Can't uninstall 'Pygments'. No files were found to uninstall.\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.19.4\n    Not uninstalling protobuf at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-e87ab28a-edc0-4502-b88f-c5254be3f0f2\n    Can't uninstall 'protobuf'. No files were found to uninstall.\n  Attempting uninstall: google-auth\n    Found existing installation: google-auth 1.33.0\n    Not uninstalling google-auth at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-e87ab28a-edc0-4502-b88f-c5254be3f0f2\n    Can't uninstall 'google-auth'. No files were found to uninstall.\n  Attempting uninstall: botocore\n    Found existing installation: botocore 1.27.28\n    Not uninstalling botocore at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-e87ab28a-edc0-4502-b88f-c5254be3f0f2\n    Can't uninstall 'botocore'. No files were found to uninstall.\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.16.0\n    Not uninstalling accelerate at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-e87ab28a-edc0-4502-b88f-c5254be3f0f2\n    Can't uninstall 'accelerate'. No files were found to uninstall.\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.26.1\n    Not uninstalling transformers at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-e87ab28a-edc0-4502-b88f-c5254be3f0f2\n    Can't uninstall 'transformers'. No files were found to uninstall.\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatabricks-feature-store 0.11.1 requires pyspark<4,>=3.1.2, which is not installed.\ntensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\ndatabricks-feature-store 0.11.1 requires pyyaml<7,>=6, but you have pyyaml 5.4.1 which is incompatible.\nboto3 1.24.28 requires botocore<1.28.0,>=1.27.28, but you have botocore 1.29.151 which is incompatible.\nSuccessfully installed SQLAlchemy-2.0.15 accelerate-0.18.0 aiohttp-cors-0.7.0 awscli-1.27.151 blessed-1.20.0 botocore-1.29.151 colorama-0.4.4 colorful-0.5.5 dataclasses-json-0.5.7 deepspeed-0.8.3 dm-tree-0.1.8 docutils-0.16 google-api-core-2.11.0 google-auth-2.19.1 gpustat-1.1 greenlet-2.0.2 gymnasium-0.26.3 gymnasium-notices-0.0.1 hjson-3.1.0 imageio-2.31.0 langchain-0.0.195 langchainplus-sdk-0.0.8 lazy_loader-0.2 lz4-4.3.2 markdown-it-py-2.2.0 marshmallow-3.19.0 marshmallow-enum-1.5.1 mdurl-0.1.2 msgpack-1.0.5 ninja-1.11.1 numexpr-2.8.4 nvidia-ml-py-11.525.112 openapi-schema-pydantic-1.2.4 opencensus-0.11.2 opencensus-context-0.1.3 prometheus-client-0.17.0 protobuf-3.20.3 py-cpuinfo-9.0.0 py-spy-0.3.14 pygments-2.15.1 pyyaml-5.4.1 ray-2.5.0 rich-13.4.1 rsa-4.7.2 scikit-image-0.21.0 tenacity-8.2.2 tensorboardX-2.6 tifffile-2023.4.12 transformers-4.28.1 typing-inspect-0.9.0\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "#install dependencies\n",
    "%pip install -r requirement.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae7a93c7-516a-4443-8b53-66a319de26c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import all the packages\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "import subprocess\n",
    "import mlflow\n",
    "\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict, List, Tuple, Union\n",
    "\n",
    "import ray\n",
    "from ray.air import session\n",
    "import ray.util.scheduling_strategies\n",
    "from ray.train.huggingface import HuggingFaceTrainer\n",
    "from ray.air.integrations.mlflow import MLflowLoggerCallback\n",
    "from ray.air.config import ScalingConfig\n",
    "\n",
    "from ray.data.preprocessors import Chain\n",
    "from ray.util.spark import setup_ray_cluster, shutdown_ray_cluster,MAX_NUM_WORKER_NODES\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "from huggingface_hub import snapshot_download\n",
    "from datasets import load_dataset,load_from_disk\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch \n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    PreTrainedTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca1a2331-89d2-4a74-b2b9-51469da67423",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Define variables (can be added as databricks widgets as well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aba74ab4-9be5-4408-8355-86bf2f83b336",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pretrained_model_name_or_path = \"tiiuae/falcon-7b\"\n",
    "dataset_path= \"teknium/GPT4-LLM-Cleaned\"\n",
    "dataset_type =  \"alpaca:chat\"\n",
    "use_gpu = True\n",
    "num_workers = 4 # Configure based on the total gpus across the worker node\n",
    "num_cpu_cores_per_worker = 6 # total cpu's present in each node\n",
    "num_gpu_per_worker = 1 # total gpu's present in each node\n",
    "max_length = 2048\n",
    "local_output_dir = '/tmp/run/details'\n",
    "gradient_checkpointing = True\n",
    "seed = 5432 \n",
    "username = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().apply('user')\n",
    "experiment_location = f\"/Users/{username}/dolly_multi-gpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1121683e-acc1-40a2-91ec-fc8c9bfbf08c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# shutdown_ray_cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ebc9876-ca62-459b-bcd9-d1d15c447f8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[11:53:04] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Ray head hostname <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">10.68.186.210</span>, port <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9569</span>                                  <a href=\"file:///local_disk0/.ephemeral_nfs/envs/pythonEnv-e87ab28a-edc0-4502-b88f-c5254be3f0f2/lib/python3.10/site-packages/ray/util/spark/cluster_init.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">cluster_init.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///local_disk0/.ephemeral_nfs/envs/pythonEnv-e87ab28a-edc0-4502-b88f-c5254be3f0f2/lib/python3.10/site-packages/ray/util/spark/cluster_init.py#455\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">455</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[2;36m[11:53:04]\u001B[0m\u001B[2;36m \u001B[0m\u001B[34mINFO    \u001B[0m Ray head hostname \u001B[1;92m10.68.186.210\u001B[0m, port \u001B[1;36m9569\u001B[0m                                  \u001B]8;id=789946;file:///local_disk0/.ephemeral_nfs/envs/pythonEnv-e87ab28a-edc0-4502-b88f-c5254be3f0f2/lib/python3.10/site-packages/ray/util/spark/cluster_init.py\u001B\\\u001B[2mcluster_init.py\u001B[0m\u001B]8;;\u001B\\\u001B[2m:\u001B[0m\u001B]8;id=44404;file:///local_disk0/.ephemeral_nfs/envs/pythonEnv-e87ab28a-edc0-4502-b88f-c5254be3f0f2/lib/python3.10/site-packages/ray/util/spark/cluster_init.py#455\u001B\\\u001B[2m455\u001B[0m\u001B]8;;\u001B\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Starting Ray head, command:                                                 <a href=\"file:///local_disk0/.ephemeral_nfs/envs/pythonEnv-e87ab28a-edc0-4502-b88f-c5254be3f0f2/lib/python3.10/site-packages/ray/util/spark/cluster_init.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">cluster_init.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///local_disk0/.ephemeral_nfs/envs/pythonEnv-e87ab28a-edc0-4502-b88f-c5254be3f0f2/lib/python3.10/site-packages/ray/util/spark/cluster_init.py#489\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">489</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>         <span style=\"color: #800080; text-decoration-color: #800080\">/local_disk0/.ephemeral_nfs/envs/pythonEnv-e87ab28a-edc0-4502-b88f-c5254be3</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                   </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>         <span style=\"color: #800080; text-decoration-color: #800080\">f0f2/bin/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">python</span> -m ray.util.spark.start_ray_node                            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                   </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>         --temp-<span style=\"color: #808000; text-decoration-color: #808000\">dir</span>=<span style=\"color: #800080; text-decoration-color: #800080\">/local_disk0/tmp/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">ray-9569-7da33b61</span> --block --head                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                   </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>         --node-ip-<span style=\"color: #808000; text-decoration-color: #808000\">address</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">10</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">.68.186.210</span> --<span style=\"color: #808000; text-decoration-color: #808000\">port</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9569</span> --num-<span style=\"color: #808000; text-decoration-color: #808000\">cpus</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> --num-<span style=\"color: #808000; text-decoration-color: #808000\">gpus</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                   </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>         --<span style=\"color: #808000; text-decoration-color: #808000\">memory</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">134217728</span> --object-store-<span style=\"color: #808000; text-decoration-color: #808000\">memory</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">134217728</span> --dashboard-<span style=\"color: #808000; text-decoration-color: #808000\">host</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">0</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">.0.0.0</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                   </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>         --dashboard-<span style=\"color: #808000; text-decoration-color: #808000\">port</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9380</span> --dashboard-agent-listen-<span style=\"color: #808000; text-decoration-color: #808000\">port</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9552</span>                    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[2;36m          \u001B[0m\u001B[2;36m \u001B[0m\u001B[34mINFO    \u001B[0m Starting Ray head, command:                                                 \u001B]8;id=156856;file:///local_disk0/.ephemeral_nfs/envs/pythonEnv-e87ab28a-edc0-4502-b88f-c5254be3f0f2/lib/python3.10/site-packages/ray/util/spark/cluster_init.py\u001B\\\u001B[2mcluster_init.py\u001B[0m\u001B]8;;\u001B\\\u001B[2m:\u001B[0m\u001B]8;id=819069;file:///local_disk0/.ephemeral_nfs/envs/pythonEnv-e87ab28a-edc0-4502-b88f-c5254be3f0f2/lib/python3.10/site-packages/ray/util/spark/cluster_init.py#489\u001B\\\u001B[2m489\u001B[0m\u001B]8;;\u001B\\\n",
       "\u001B[2;36m           \u001B[0m         \u001B[35m/local_disk0/.ephemeral_nfs/envs/pythonEnv-e87ab28a-edc0-4502-b88f-c5254be3\u001B[0m \u001B[2m                   \u001B[0m\n",
       "\u001B[2;36m           \u001B[0m         \u001B[35mf0f2/bin/\u001B[0m\u001B[95mpython\u001B[0m -m ray.util.spark.start_ray_node                            \u001B[2m                   \u001B[0m\n",
       "\u001B[2;36m           \u001B[0m         --temp-\u001B[33mdir\u001B[0m=\u001B[35m/local_disk0/tmp/\u001B[0m\u001B[95mray-9569-7da33b61\u001B[0m --block --head                \u001B[2m                   \u001B[0m\n",
       "\u001B[2;36m           \u001B[0m         --node-ip-\u001B[33maddress\u001B[0m=\u001B[1;92m10\u001B[0m\u001B[1;92m.68.186.210\u001B[0m --\u001B[33mport\u001B[0m=\u001B[1;36m9569\u001B[0m --num-\u001B[33mcpus\u001B[0m=\u001B[1;36m0\u001B[0m --num-\u001B[33mgpus\u001B[0m=\u001B[1;36m0\u001B[0m       \u001B[2m                   \u001B[0m\n",
       "\u001B[2;36m           \u001B[0m         --\u001B[33mmemory\u001B[0m=\u001B[1;36m134217728\u001B[0m --object-store-\u001B[33mmemory\u001B[0m=\u001B[1;36m134217728\u001B[0m --dashboard-\u001B[33mhost\u001B[0m=\u001B[1;92m0\u001B[0m\u001B[1;92m.0.0.0\u001B[0m \u001B[2m                   \u001B[0m\n",
       "\u001B[2;36m           \u001B[0m         --dashboard-\u001B[33mport\u001B[0m=\u001B[1;36m9380\u001B[0m --dashboard-agent-listen-\u001B[33mport\u001B[0m=\u001B[1;36m9552\u001B[0m                    \u001B[2m                   \u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-10 11:53:06,143\tINFO usage_lib.py:408 -- Usage stats collection is enabled by default without user confirmation because this terminal is detected to be non-interactive. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage-stats` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.\n2023-06-10 11:53:06,143\tINFO scripts.py:712 -- \u001B[37mLocal node IP\u001B[39m: \u001B[1m10.68.186.210\u001B[22m\n2023-06-10 11:53:08,100\tSUCC scripts.py:749 -- \u001B[32m--------------------\u001B[39m\n2023-06-10 11:53:08,100\tSUCC scripts.py:750 -- \u001B[32mRay runtime started.\u001B[39m\n2023-06-10 11:53:08,101\tSUCC scripts.py:751 -- \u001B[32m--------------------\u001B[39m\n2023-06-10 11:53:08,101\tINFO scripts.py:753 -- \u001B[36mNext steps\u001B[39m\n2023-06-10 11:53:08,101\tINFO scripts.py:756 -- To add another node to this Ray cluster, run\n2023-06-10 11:53:08,101\tINFO scripts.py:759 -- \u001B[1m  ray start --address='10.68.186.210:9569'\u001B[22m\n2023-06-10 11:53:08,101\tINFO scripts.py:768 -- To connect to this Ray cluster:\n2023-06-10 11:53:08,101\tINFO scripts.py:770 -- \u001B[35mimport\u001B[39m\u001B[26m ray\n2023-06-10 11:53:08,101\tINFO scripts.py:771 -- ray\u001B[35m.\u001B[39m\u001B[26minit(_node_ip_address\u001B[35m=\u001B[39m\u001B[26m\u001B[33m'10.68.186.210'\u001B[39m\u001B[26m)\n2023-06-10 11:53:08,101\tINFO scripts.py:783 -- To submit a Ray job using the Ray Jobs CLI:\n2023-06-10 11:53:08,101\tINFO scripts.py:784 -- \u001B[1m  RAY_ADDRESS='http://10.68.186.210:9380' ray job submit --working-dir . -- python my_script.py\u001B[22m\n2023-06-10 11:53:08,101\tINFO scripts.py:793 -- See https://docs.ray.io/en/latest/cluster/running-applications/job-submission/index.html \n2023-06-10 11:53:08,101\tINFO scripts.py:797 -- for more information on submitting Ray jobs to the Ray cluster.\n2023-06-10 11:53:08,101\tINFO scripts.py:802 -- To terminate the Ray runtime, run\n2023-06-10 11:53:08,101\tINFO scripts.py:803 -- \u001B[1m  ray stop\u001B[22m\n2023-06-10 11:53:08,101\tINFO scripts.py:806 -- To view the status of the cluster, use\n2023-06-10 11:53:08,101\tINFO scripts.py:807 --   \u001B[1mray status\u001B[22m\u001B[26m\n2023-06-10 11:53:08,101\tINFO scripts.py:811 -- To monitor and debug Ray, view the dashboard at \n2023-06-10 11:53:08,101\tINFO scripts.py:812 --   \u001B[1m10.68.186.210:9380\u001B[22m\u001B[26m\n2023-06-10 11:53:08,101\tINFO scripts.py:819 -- \u001B[4mIf connection to the dashboard fails, check your firewall settings and network configuration.\u001B[24m\n2023-06-10 11:53:08,102\tINFO scripts.py:919 -- \u001B[36m\u001B[1m--block\u001B[22m\u001B[39m\n2023-06-10 11:53:08,102\tINFO scripts.py:920 -- This command will now block forever until terminated by a signal.\n2023-06-10 11:53:08,102\tINFO scripts.py:923 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[11:53:09] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Ray head node started.                                                      <a href=\"file:///local_disk0/.ephemeral_nfs/envs/pythonEnv-e87ab28a-edc0-4502-b88f-c5254be3f0f2/lib/python3.10/site-packages/ray/util/spark/cluster_init.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">cluster_init.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///local_disk0/.ephemeral_nfs/envs/pythonEnv-e87ab28a-edc0-4502-b88f-c5254be3f0f2/lib/python3.10/site-packages/ray/util/spark/cluster_init.py#515\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">515</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[2;36m[11:53:09]\u001B[0m\u001B[2;36m \u001B[0m\u001B[34mINFO    \u001B[0m Ray head node started.                                                      \u001B]8;id=894478;file:///local_disk0/.ephemeral_nfs/envs/pythonEnv-e87ab28a-edc0-4502-b88f-c5254be3f0f2/lib/python3.10/site-packages/ray/util/spark/cluster_init.py\u001B\\\u001B[2mcluster_init.py\u001B[0m\u001B]8;;\u001B\\\u001B[2m:\u001B[0m\u001B]8;id=252102;file:///local_disk0/.ephemeral_nfs/envs/pythonEnv-e87ab28a-edc0-4502-b88f-c5254be3f0f2/lib/python3.10/site-packages/ray/util/spark/cluster_init.py#515\u001B\\\u001B[2m515\u001B[0m\u001B]8;;\u001B\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> The Ray cluster will be shut down automatically if you don't run         <a href=\"file:///local_disk0/.ephemeral_nfs/envs/pythonEnv-e87ab28a-edc0-4502-b88f-c5254be3f0f2/lib/python3.10/site-packages/ray/util/spark/databricks_hook.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">databricks_hook.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///local_disk0/.ephemeral_nfs/envs/pythonEnv-e87ab28a-edc0-4502-b88f-c5254be3f0f2/lib/python3.10/site-packages/ray/util/spark/databricks_hook.py#132\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">132</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>         commands on the Databricks notebook for <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30.0</span> minutes. You can change the <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                      </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>         auto-shutdown minutes by setting                                         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                      </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>         <span style=\"color: #008000; text-decoration-color: #008000\">'DATABRICKS_RAY_ON_SPARK_AUTOSHUTDOWN_MINUTES'</span> environment variable,     <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                      </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>         setting it to <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> means that the Ray cluster keeps running until you       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                      </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>         manually call `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ray.util.spark.shutdown_ray_cluster</span><span style=\"font-weight: bold\">()</span>` or detach          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                      </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>         Databricks notebook.                                                     <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                      </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[2;36m          \u001B[0m\u001B[2;36m \u001B[0m\u001B[34mINFO    \u001B[0m The Ray cluster will be shut down automatically if you don't run         \u001B]8;id=816497;file:///local_disk0/.ephemeral_nfs/envs/pythonEnv-e87ab28a-edc0-4502-b88f-c5254be3f0f2/lib/python3.10/site-packages/ray/util/spark/databricks_hook.py\u001B\\\u001B[2mdatabricks_hook.py\u001B[0m\u001B]8;;\u001B\\\u001B[2m:\u001B[0m\u001B]8;id=411188;file:///local_disk0/.ephemeral_nfs/envs/pythonEnv-e87ab28a-edc0-4502-b88f-c5254be3f0f2/lib/python3.10/site-packages/ray/util/spark/databricks_hook.py#132\u001B\\\u001B[2m132\u001B[0m\u001B]8;;\u001B\\\n",
       "\u001B[2;36m           \u001B[0m         commands on the Databricks notebook for \u001B[1;36m30.0\u001B[0m minutes. You can change the \u001B[2m                      \u001B[0m\n",
       "\u001B[2;36m           \u001B[0m         auto-shutdown minutes by setting                                         \u001B[2m                      \u001B[0m\n",
       "\u001B[2;36m           \u001B[0m         \u001B[32m'DATABRICKS_RAY_ON_SPARK_AUTOSHUTDOWN_MINUTES'\u001B[0m environment variable,     \u001B[2m                      \u001B[0m\n",
       "\u001B[2;36m           \u001B[0m         setting it to \u001B[1;36m0\u001B[0m means that the Ray cluster keeps running until you       \u001B[2m                      \u001B[0m\n",
       "\u001B[2;36m           \u001B[0m         manually call `\u001B[1;35mray.util.spark.shutdown_ray_cluster\u001B[0m\u001B[1m(\u001B[0m\u001B[1m)\u001B[0m` or detach          \u001B[2m                      \u001B[0m\n",
       "\u001B[2;36m           \u001B[0m         Databricks notebook.                                                     \u001B[2m                      \u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-10 11:53:39,939\tINFO worker.py:1452 -- Connecting to existing Ray cluster at address: 10.68.186.210:9569...\n2023-06-10 11:53:39,951\tINFO worker.py:1627 -- Connected to Ray cluster. View the dashboard at \u001B[1m\u001B[32mhttp://10.68.186.210:9380 \u001B[39m\u001B[22m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To monitor and debug Ray from Databricks, view the dashboard at \n https://dbc-dp-6051921418418893.cloud.databricks.com/driver-proxy/o/6051921418418893/0603-224203-e80gcfmk/9380/\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "      <div style=\"margin-bottom: 16px\">\n",
       "          <a href=\"/driver-proxy/o/6051921418418893/0603-224203-e80gcfmk/9380/\">\n",
       "              Open Ray Cluster Dashboard in a new tab\n",
       "          </a>\n",
       "      </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'10.68.186.210:9569'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start the ray cluster\n",
    "setup_ray_cluster(\n",
    "  num_worker_nodes=MAX_NUM_WORKER_NODES,\n",
    "  num_cpus_per_node=num_cpu_cores_per_worker,\n",
    "  num_gpus_per_node=num_gpu_per_worker,\n",
    "  collect_log_to_path=\"/dbfs/path/to/ray_collected_logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a38445c1-a929-49a6-af30-1f3db74ebc21",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We will use `ray.init()` to initialize the ray cluster in the current session.\n",
    "\n",
    "We define a `runtime_env` to ensure that the Ray workers have access to all the necessary packages. You can omit the `runtime_env` argument if you have all of the packages already installed on each node in your cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be6f38a3-d93d-4839-a9a8-c67fee3b91b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-10 11:53:44,308\tINFO worker.py:1334 -- Using address 10.68.186.210:9569 set in the environment variable RAY_ADDRESS\n2023-06-10 11:53:44,309\tINFO worker.py:1452 -- Connecting to existing Ray cluster at address: 10.68.186.210:9569...\n2023-06-10 11:53:44,315\tINFO worker.py:1627 -- Connected to Ray cluster. View the dashboard at \u001B[1m\u001B[32mhttp://10.68.186.210:9380 \u001B[39m\u001B[22m\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <h3 style=\"color: var(--jp-ui-font-color0)\">Ray</h3>\n",
       "        <svg version=\"1.1\" id=\"ray\" width=\"3em\" viewBox=\"0 0 144.5 144.6\" style=\"margin-left: 3em;margin-right: 3em\">\n",
       "            <g id=\"layer-1\">\n",
       "                <path fill=\"#00a2e9\" class=\"st0\" d=\"M97.3,77.2c-3.8-1.1-6.2,0.9-8.3,5.1c-3.5,6.8-9.9,9.9-17.4,9.6S58,88.1,54.8,81.2c-1.4-3-3-4-6.3-4.1\n",
       "                    c-5.6-0.1-9.9,0.1-13.1,6.4c-3.8,7.6-13.6,10.2-21.8,7.6C5.2,88.4-0.4,80.5,0,71.7c0.1-8.4,5.7-15.8,13.8-18.2\n",
       "                    c8.4-2.6,17.5,0.7,22.3,8c1.3,1.9,1.3,5.2,3.6,5.6c3.9,0.6,8,0.2,12,0.2c1.8,0,1.9-1.6,2.4-2.8c3.5-7.8,9.7-11.8,18-11.9\n",
       "                    c8.2-0.1,14.4,3.9,17.8,11.4c1.3,2.8,2.9,3.6,5.7,3.3c1-0.1,2,0.1,3,0c2.8-0.5,6.4,1.7,8.1-2.7s-2.3-5.5-4.1-7.5\n",
       "                    c-5.1-5.7-10.9-10.8-16.1-16.3C84,38,81.9,37.1,78,38.3C66.7,42,56.2,35.7,53,24.1C50.3,14,57.3,2.8,67.7,0.5\n",
       "                    C78.4-2,89,4.7,91.5,15.3c0.1,0.3,0.1,0.5,0.2,0.8c0.7,3.4,0.7,6.9-0.8,9.8c-1.7,3.2-0.8,5,1.5,7.2c6.7,6.5,13.3,13,19.8,19.7\n",
       "                    c1.8,1.8,3,2.1,5.5,1.2c9.1-3.4,17.9-0.6,23.4,7c4.8,6.9,4.6,16.1-0.4,22.9c-5.4,7.2-14.2,9.9-23.1,6.5c-2.3-0.9-3.5-0.6-5.1,1.1\n",
       "                    c-6.7,6.9-13.6,13.7-20.5,20.4c-1.8,1.8-2.5,3.2-1.4,5.9c3.5,8.7,0.3,18.6-7.7,23.6c-7.9,5-18.2,3.8-24.8-2.9\n",
       "                    c-6.4-6.4-7.4-16.2-2.5-24.3c4.9-7.8,14.5-11,23.1-7.8c3,1.1,4.7,0.5,6.9-1.7C91.7,98.4,98,92.3,104.2,86c1.6-1.6,4.1-2.7,2.6-6.2\n",
       "                    c-1.4-3.3-3.8-2.5-6.2-2.6C99.8,77.2,98.9,77.2,97.3,77.2z M72.1,29.7c5.5,0.1,9.9-4.3,10-9.8c0-0.1,0-0.2,0-0.3\n",
       "                    C81.8,14,77,9.8,71.5,10.2c-5,0.3-9,4.2-9.3,9.2c-0.2,5.5,4,10.1,9.5,10.3C71.8,29.7,72,29.7,72.1,29.7z M72.3,62.3\n",
       "                    c-5.4-0.1-9.9,4.2-10.1,9.7c0,0.2,0,0.3,0,0.5c0.2,5.4,4.5,9.7,9.9,10c5.1,0.1,9.9-4.7,10.1-9.8c0.2-5.5-4-10-9.5-10.3\n",
       "                    C72.6,62.3,72.4,62.3,72.3,62.3z M115,72.5c0.1,5.4,4.5,9.7,9.8,9.9c5.6-0.2,10-4.8,10-10.4c-0.2-5.4-4.6-9.7-10-9.7\n",
       "                    c-5.3-0.1-9.8,4.2-9.9,9.5C115,72.1,115,72.3,115,72.5z M19.5,62.3c-5.4,0.1-9.8,4.4-10,9.8c-0.1,5.1,5.2,10.4,10.2,10.3\n",
       "                    c5.6-0.2,10-4.9,9.8-10.5c-0.1-5.4-4.5-9.7-9.9-9.6C19.6,62.3,19.5,62.3,19.5,62.3z M71.8,134.6c5.9,0.2,10.3-3.9,10.4-9.6\n",
       "                    c0.5-5.5-3.6-10.4-9.1-10.8c-5.5-0.5-10.4,3.6-10.8,9.1c0,0.5,0,0.9,0,1.4c-0.2,5.3,4,9.8,9.3,10\n",
       "                    C71.6,134.6,71.7,134.6,71.8,134.6z\"/>\n",
       "            </g>\n",
       "        </svg>\n",
       "        <table>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "                <td style=\"text-align: left\"><b>3.10.6</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "                <td style=\"text-align: left\"><b> 2.5.0</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "    <td style=\"text-align: left\"><b>Dashboard:</b></td>\n",
       "    <td style=\"text-align: left\"><b><a href=\"http://10.68.186.210:9380\" target=\"_blank\">http://10.68.186.210:9380</a></b></td>\n",
       "</tr>\n",
       "\n",
       "        </table>\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='10.68.186.210:9380', python_version='3.10.6', ray_version='2.5.0', ray_commit='586c376e0769082cb5cfa1333e8264a5fa6b73ec', address_info={'node_ip_address': '10.68.186.210', 'raylet_ip_address': '10.68.186.210', 'redis_address': None, 'object_store_address': '/local_disk0/tmp/ray-9569-7da33b61/session_2023-06-10_11-53-06_145059_239589/sockets/plasma_store', 'raylet_socket_name': '/local_disk0/tmp/ray-9569-7da33b61/session_2023-06-10_11-53-06_145059_239589/sockets/raylet', 'webui_url': '10.68.186.210:9380', 'session_dir': '/tmp/ray/session_2023-06-10_11-53-06_145059_239589', 'metrics_export_port': 48293, 'gcs_address': '10.68.186.210:9569', 'address': '10.68.186.210:9569', 'dashboard_agent_listen_port': 52365, 'node_id': 'd42a6c59792cfec19e07cc0e64ae6f8e9f09033843c822943d67d896'})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runtime_env = {\n",
    "    \"env_vars\": {\"RAY_memory_monitor_refresh_ms\": \"0\"}\n",
    "}\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fab8cd40-9954-4297-9d63-aea859bf6562",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "we will catch the models in the local nodes to avoid getting it from HF Server everytime during training calling the `snapshot_download()` command from HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fb1e910-9ccd-477f-895a-c735b4a91608",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(download_model pid=6992, ip=10.68.175.10)\u001B[0m \rFetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]\n\u001B[2m\u001B[36m(download_model pid=6886, ip=10.68.178.177)\u001B[0m \n\u001B[2m\u001B[36m(download_model pid=6886, ip=10.68.178.177)\u001B[0m \rDownloading (…)/configuration_RW.py:   0%|          | 0.00/2.61k [00:00<?, ?B/s]\u001B[A\rDownloading (…)/configuration_RW.py: 100%|██████████| 2.61k/2.61k [00:00<00:00, 1.58MB/s]\n\u001B[2m\u001B[36m(download_model pid=6964, ip=10.68.174.155)\u001B[0m \rDownloading (…)35e5bf6ee5/README.md:   0%|          | 0.00/10.2k [00:00<?, ?B/s]\u001B[A\rDownloading (…)35e5bf6ee5/README.md: 100%|██████████| 10.2k/10.2k [00:00<00:00, 5.77MB/s]\n\u001B[2m\u001B[36m(download_model pid=6886, ip=10.68.178.177)\u001B[0m \rFetching 12 files:   8%|▊         | 1/12 [00:02<00:25,  2.31s/it]\n\u001B[2m\u001B[36m(download_model pid=6992, ip=10.68.175.10)\u001B[0m \rDownloading (…)6ee5/modelling_RW.py:   0%|          | 0.00/47.6k [00:00<?, ?B/s]\u001B[A\n\u001B[2m\u001B[36m(download_model pid=6992, ip=10.68.175.10)\u001B[0m \rDownloading (…)6ee5/modelling_RW.py: 100%|██████████| 47.6k/47.6k [00:00<00:00, 635kB/s]\n\u001B[2m\u001B[36m(download_model pid=6992, ip=10.68.175.10)\u001B[0m \rDownloading (…)l-00001-of-00002.bin:   0%|          | 0.00/9.95G [00:00<?, ?B/s]\u001B[A\n\u001B[2m\u001B[36m(download_model pid=6992, ip=10.68.175.10)\u001B[0m \rDownloading (…)l-00002-of-00002.bin:   0%|          | 0.00/4.48G [00:00<?, ?B/s]\u001B[A\u001B[A\n\u001B[2m\u001B[36m(download_model pid=6992, ip=10.68.175.10)\u001B[0m \rDownloading (…)f6ee5/.gitattributes:   0%|          | 0.00/1.48k [00:00<?, ?B/s]\u001B[A\u001B[A\u001B[A\rDownloading (…)f6ee5/.gitattributes: 100%|██████████| 1.48k/1.48k [00:00<00:00, 886kB/s]\n\u001B[2m\u001B[36m(download_model pid=6992, ip=10.68.175.10)\u001B[0m \rDownloading (…)/configuration_RW.py:   0%|          | 0.00/2.61k [00:00<?, ?B/s]\u001B[A\u001B[A\u001B[A\rDownloading (…)/configuration_RW.py: 100%|██████████| 2.61k/2.61k [00:00<00:00, 1.83MB/s]\n\u001B[2m\u001B[36m(download_model pid=6886, ip=10.68.178.177)\u001B[0m \rDownloading (…)6ee5/modelling_RW.py:   0%|          | 0.00/47.6k [00:00<?, ?B/s]\u001B[A\u001B[A\u001B[A\n\u001B[2m\u001B[36m(download_model pid=6886, ip=10.68.178.177)\u001B[0m \rDownloading (…)neration_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]\u001B[A\u001B[A\u001B[A\u001B[A\rDownloading (…)neration_config.json: 100%|██████████| 111/111 [00:00<00:00, 50.2kB/s]\n\u001B[2m\u001B[36m(download_model pid=6964, ip=10.68.174.155)\u001B[0m \rDownloading (…)neration_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]\u001B[A\u001B[A\u001B[A\rDownloading (…)neration_config.json: 100%|██████████| 111/111 [00:00<00:00, 68.0kB/s]\n\u001B[2m\u001B[36m(download_model pid=7003, ip=10.68.166.174)\u001B[0m \rDownloading (…)neration_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]\u001B[A\rDownloading (…)neration_config.json: 100%|██████████| 111/111 [00:00<00:00, 83.4kB/s]\n\u001B[2m\u001B[36m(download_model pid=6992, ip=10.68.175.10)\u001B[0m \rDownloading (…)35e5bf6ee5/README.md:   0%|          | 0.00/10.2k [00:00<?, ?B/s]\u001B[A\u001B[A\u001B[A\rDownloading (…)35e5bf6ee5/README.md: 100%|██████████| 10.2k/10.2k [00:00<00:00, 5.25MB/s]\n\u001B[2m\u001B[36m(download_model pid=6992, ip=10.68.175.10)\u001B[0m \rDownloading (…)l-00002-of-00002.bin:   0%|          | 10.5M/4.48G [00:00<00:47, 93.3MB/s]\u001B[A\u001B[A\n\u001B[2m\u001B[36m(download_model pid=6886, ip=10.68.178.177)\u001B[0m \rDownloading (…)35e5bf6ee5/README.md:   0%|          | 0.00/10.2k [00:00<?, ?B/s]\u001B[A\u001B[A\u001B[A\u001B[A\rDownloading (…)35e5bf6ee5/README.md: 100%|██████████| 10.2k/10.2k [00:00<00:00, 7.39MB/s]\n\u001B[2m\u001B[36m(download_model pid=6886, ip=10.68.178.177)\u001B[0m \rDownloading (…)e5bf6ee5/config.json:   0%|          | 0.00/950 [00:00<?, ?B/s]\u001B[A\u001B[A\u001B[A\u001B[A\rDownloading (…)e5bf6ee5/config.json: 100%|██████████| 950/950 [00:00<00:00, 251kB/s]\n\u001B[2m\u001B[36m(download_model pid=6964, ip=10.68.174.155)\u001B[0m \rDownloading (…)/configuration_RW.py:   0%|          | 0.00/2.61k [00:00<?, ?B/s]\u001B[A\u001B[A\u001B[A\u001B[A\rDownloading (…)/configuration_RW.py: 100%|██████████| 2.61k/2.61k [00:00<00:00, 1.71MB/s]\n\u001B[2m\u001B[36m(download_model pid=6964, ip=10.68.174.155)\u001B[0m \rDownloading (…)l-00002-of-00002.bin:   0%|          | 10.5M/4.48G [00:00<00:48, 92.1MB/s]\u001B[A\rDownloading (…)6ee5/modelling_RW.py: 100%|██████████| 47.6k/47.6k [00:00<00:00, 681kB/s]\n\u001B[2m\u001B[36m(download_model pid=6886, ip=10.68.178.177)\u001B[0m \rDownloading (…)l-00001-of-00002.bin:   0%|          | 31.5M/9.95G [00:00<01:07, 147MB/s] \u001B[A\n\u001B[2m\u001B[36m(download_model pid=6964, ip=10.68.174.155)\u001B[0m \rDownloading (…)l-00001-of-00002.bin:   0%|          | 31.5M/9.95G [00:00<01:14, 133MB/s] \u001B[A\u001B[A\n\u001B[2m\u001B[36m(download_model pid=6886, ip=10.68.178.177)\u001B[0m \rDownloading (…)cial_tokens_map.json:   0%|          | 0.00/281 [00:00<?, ?B/s]\u001B[A\u001B[A\u001B[A\rDownloading (…)cial_tokens_map.json: 100%|██████████| 281/281 [00:00<00:00, 155kB/s]\n\u001B[2m\u001B[36m(download_model pid=6886, ip=10.68.178.177)\u001B[0m \rDownloading (…)okenizer_config.json:   0%|          | 0.00/220 [00:00<?, ?B/s]\u001B[A\u001B[A\u001B[A\n\u001B[2m\u001B[36m(download_model pid=6886, ip=10.68.178.177)\u001B[0m \rDownloading (…)okenizer_config.json: 100%|██████████| 220/220 [00:00<00:00, 195kB/s]\n\u001B[2m\u001B[36m(download_model pid=6964, ip=10.68.174.155)\u001B[0m \rDownloading (…)model.bin.index.json:   0%|          | 0.00/16.9k [00:00<?, ?B/s]\u001B[A\u001B[A\u001B[A\rDownloading (…)model.bin.index.json: 100%|██████████| 16.9k/16.9k [00:00<00:00, 5.34MB/s]\n\u001B[2m\u001B[36m(download_model pid=6964, ip=10.68.174.155)\u001B[0m \rDownloading (…)okenizer_config.json:   0%|          | 0.00/220 [00:00<?, ?B/s]\u001B[A\u001B[A\u001B[A\rDownloading (…)okenizer_config.json: 100%|██████████| 220/220 [00:00<00:00, 155kB/s]\n\u001B[2m\u001B[36m(download_model pid=6992, ip=10.68.175.10)\u001B[0m \rDownloading (…)f6ee5/tokenizer.json: 100%|██████████| 2.73M/2.73M [00:00<00:00, 18.3MB/s]\u001B[A\u001B[A\u001B[A\rDownloading (…)f6ee5/tokenizer.json: 100%|██████████| 2.73M/2.73M [00:00<00:00, 18.0MB/s]\n\u001B[2m\u001B[36m(download_model pid=7003, ip=10.68.166.174)\u001B[0m \rFetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]\u001B[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=6992, ip=10.68.175.10)\u001B[0m \u001B[32m [repeated 381x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=7003, ip=10.68.166.174)\u001B[0m \rDownloading (…)f6ee5/.gitattributes:   0%|          | 0.00/1.48k [00:00<?, ?B/s]\u001B[A\rDownloading (…)f6ee5/.gitattributes: 100%|██████████| 1.48k/1.48k [00:00<00:00, 1.31MB/s]\u001B[32m [repeated 5x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=7003, ip=10.68.166.174)\u001B[0m \rFetching 12 files:  17%|█▋        | 2/12 [00:02<00:11,  1.12s/it]\u001B[32m [repeated 7x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=7003, ip=10.68.166.174)\u001B[0m \rDownloading (…)l-00002-of-00002.bin:   0%|          | 0.00/4.48G [00:00<?, ?B/s]\u001B[A\u001B[32m [repeated 3x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=6886, ip=10.68.178.177)\u001B[0m \rDownloading (…)l-00001-of-00002.bin:   9%|▊         | 860M/9.95G [00:05<00:48, 187MB/s]\u001B[A\n\u001B[2m\u001B[36m(download_model pid=7003, ip=10.68.166.174)\u001B[0m \rDownloading (…)l-00001-of-00002.bin:   0%|          | 0.00/9.95G [00:00<?, ?B/s]\u001B[A\u001B[A\u001B[32m [repeated 3x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=7003, ip=10.68.166.174)\u001B[0m \rDownloading (…)35e5bf6ee5/README.md:   0%|          | 0.00/10.2k [00:00<?, ?B/s]\u001B[A\u001B[A\u001B[A\rDownloading (…)35e5bf6ee5/README.md: 100%|██████████| 10.2k/10.2k [00:00<00:00, 5.86MB/s]\u001B[32m [repeated 2x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=7003, ip=10.68.166.174)\u001B[0m \rDownloading (…)l-00001-of-00002.bin:   9%|▊         | 860M/9.95G [00:05<00:48, 187MB/s]\u001B[A\n\u001B[2m\u001B[36m(download_model pid=6964, ip=10.68.174.155)\u001B[0m \rDownloading (…)f6ee5/tokenizer.json:   0%|          | 0.00/2.73M [00:00<?, ?B/s]\u001B[A\u001B[A\u001B[A\u001B[32m [repeated 6x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=6992, ip=10.68.175.10)\u001B[0m \rDownloading (…)l-00001-of-00002.bin:   9%|▊         | 860M/9.95G [00:05<00:48, 187MB/s]\u001B[A\n\u001B[2m\u001B[36m(download_model pid=6992, ip=10.68.175.10)\u001B[0m \rDownloading (…)l-00001-of-00002.bin:   9%|▉         | 881M/9.95G [00:05<00:48, 189MB/s]\u001B[A\u001B[32m [repeated 311x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=7003, ip=10.68.166.174)\u001B[0m \rDownloading (…)l-00001-of-00002.bin:   9%|▉         | 881M/9.95G [00:05<00:48, 189MB/s]\u001B[A\n\u001B[2m\u001B[36m(download_model pid=6964, ip=10.68.174.155)\u001B[0m \rDownloading (…)l-00002-of-00002.bin:   4%|▍         | 178M/4.48G [00:01<00:43, 100MB/s] \u001B[A\u001B[32m [repeated 7x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=6886, ip=10.68.178.177)\u001B[0m \rDownloading (…)l-00002-of-00002.bin:  10%|█         | 461M/4.48G [00:04<00:40, 100MB/s] \u001B[A\u001B[A\u001B[32m [repeated 8x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=7003, ip=10.68.166.174)\u001B[0m \rDownloading (…)cial_tokens_map.json:   0%|          | 0.00/281 [00:00<?, ?B/s]\u001B[A\u001B[A\u001B[A\rDownloading (…)cial_tokens_map.json: 100%|██████████| 281/281 [00:00<00:00, 137kB/s]\u001B[32m [repeated 3x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=6886, ip=10.68.178.177)\u001B[0m \rDownloading (…)model.bin.index.json:   0%|          | 0.00/16.9k [00:00<?, ?B/s]\u001B[A\u001B[A\u001B[A\rDownloading (…)model.bin.index.json: 100%|██████████| 16.9k/16.9k [00:00<00:00, 11.6MB/s]\u001B[32m [repeated 3x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=7003, ip=10.68.166.174)\u001B[0m \rDownloading (…)okenizer_config.json:   0%|          | 0.00/220 [00:00<?, ?B/s]\u001B[A\u001B[A\u001B[A\rDownloading (…)okenizer_config.json: 100%|██████████| 220/220 [00:00<00:00, 127kB/s]\u001B[32m [repeated 2x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=6964, ip=10.68.174.155)\u001B[0m \rDownloading (…)f6ee5/tokenizer.json: 100%|██████████| 2.73M/2.73M [00:00<00:00, 6.38MB/s]\u001B[A\u001B[A\u001B[A\rDownloading (…)f6ee5/tokenizer.json: 100%|██████████| 2.73M/2.73M [00:00<00:00, 6.34MB/s]\u001B[32m [repeated 3x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=6992, ip=10.68.175.10)\u001B[0m \u001B[32m [repeated 363x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=6992, ip=10.68.175.10)\u001B[0m \rDownloading (…)l-00001-of-00002.bin:  16%|█▌        | 1.60G/9.95G [00:10<00:43, 192MB/s]\u001B[A\u001B[32m [repeated 296x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=6886, ip=10.68.178.177)\u001B[0m \rDownloading (…)l-00001-of-00002.bin:  15%|█▍        | 1.48G/9.95G [00:09<01:14, 114MB/s] \u001B[A\u001B[32m [repeated 7x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=7003, ip=10.68.166.174)\u001B[0m \rDownloading (…)l-00001-of-00002.bin:  15%|█▍        | 1.47G/9.95G [00:09<01:16, 111MB/s] \u001B[A\u001B[A\u001B[32m [repeated 7x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=6964, ip=10.68.174.155)\u001B[0m \u001B[32m [repeated 355x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=6964, ip=10.68.174.155)\u001B[0m \rDownloading (…)l-00001-of-00002.bin:  23%|██▎       | 2.32G/9.95G [00:15<00:41, 182MB/s]\u001B[A\u001B[A\u001B[32m [repeated 294x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=6886, ip=10.68.178.177)\u001B[0m \rDownloading (…)l-00001-of-00002.bin:  20%|█▉        | 1.95G/9.95G [00:13<01:16, 104MB/s] \u001B[A\u001B[32m [repeated 4x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=6992, ip=10.68.175.10)\u001B[0m \rDownloading (…)l-00002-of-00002.bin:  33%|███▎      | 1.48G/4.48G [00:15<00:30, 100MB/s] \u001B[A\u001B[A\u001B[32m [repeated 5x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=6964, ip=10.68.174.155)\u001B[0m \rFetching 12 files:  42%|████▏     | 5/12 [00:19<00:02,  2.74it/s]\n\u001B[2m\u001B[36m(download_model pid=6964, ip=10.68.174.155)\u001B[0m \u001B[32m [repeated 339x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=6964, ip=10.68.174.155)\u001B[0m \rDownloading (…)l-00002-of-00002.bin:  44%|████▍     | 1.99G/4.48G [00:20<00:25, 99.1MB/s]\u001B[A\u001B[32m [repeated 268x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=6964, ip=10.68.174.155)\u001B[0m \rDownloading (…)l-00002-of-00002.bin:  44%|████▎     | 1.96G/4.48G [00:20<00:25, 100MB/s] \u001B[A\u001B[32m [repeated 10x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=7003, ip=10.68.166.174)\u001B[0m \rDownloading (…)l-00001-of-00002.bin:  29%|██▉       | 2.89G/9.95G [00:19<01:04, 110MB/s] \u001B[A\u001B[A\u001B[32m [repeated 13x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=6886, ip=10.68.178.177)\u001B[0m \u001B[32m [repeated 325x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=7003, ip=10.68.166.174)\u001B[0m \rDownloading (…)l-00002-of-00002.bin:  55%|█████▌    | 2.47G/4.48G [00:25<00:20, 98.8MB/s]\u001B[A\u001B[32m [repeated 260x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=6886, ip=10.68.178.177)\u001B[0m \rDownloading (…)l-00001-of-00002.bin:  36%|███▌      | 3.59G/9.95G [00:25<00:53, 119MB/s] \u001B[A\u001B[32m [repeated 10x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=6964, ip=10.68.174.155)\u001B[0m \rDownloading (…)l-00001-of-00002.bin:  36%|███▌      | 3.57G/9.95G [00:25<01:03, 101MB/s] \u001B[A\u001B[A\u001B[32m [repeated 10x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=6992, ip=10.68.175.10)\u001B[0m \u001B[32m [repeated 328x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=6886, ip=10.68.178.177)\u001B[0m \rDownloading (…)l-00001-of-00002.bin:  41%|████▏     | 4.11G/9.95G [00:30<01:26, 67.3MB/s]\u001B[A\u001B[32m [repeated 269x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=7003, ip=10.68.166.174)\u001B[0m \rDownloading (…)l-00002-of-00002.bin:  58%|█████▊    | 2.62G/4.48G [00:26<00:17, 109MB/s] \u001B[A\u001B[32m [repeated 6x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=6886, ip=10.68.178.177)\u001B[0m \rDownloading (…)l-00002-of-00002.bin:  62%|██████▏   | 2.78G/4.48G [00:28<00:16, 101MB/s] \u001B[A\u001B[A\u001B[32m [repeated 6x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=7003, ip=10.68.166.174)\u001B[0m \u001B[32m [repeated 301x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=7003, ip=10.68.166.174)\u001B[0m \rDownloading (…)l-00002-of-00002.bin:  73%|███████▎  | 3.27G/4.48G [00:35<00:18, 66.8MB/s]\u001B[A\u001B[32m [repeated 260x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=6886, ip=10.68.178.177)\u001B[0m \u001B[32m [repeated 298x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=7003, ip=10.68.166.174)\u001B[0m \u001B[A\n\u001B[2m\u001B[36m(download_model pid=6886, ip=10.68.178.177)\u001B[0m \rDownloading (…)l-00002-of-00002.bin:  81%|████████  | 3.63G/4.48G [00:40<00:12, 67.6MB/s]\u001B[A\u001B[A\u001B[32m [repeated 255x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=7003, ip=10.68.166.174)\u001B[0m \u001B[32m [repeated 311x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=7003, ip=10.68.166.174)\u001B[0m \rDownloading (…)l-00001-of-00002.bin:  52%|█████▏    | 5.14G/9.95G [00:45<01:09, 69.2MB/s]\u001B[A\u001B[A\u001B[32m [repeated 271x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=6964, ip=10.68.174.155)\u001B[0m \rDownloading (…)l-00002-of-00002.bin:  91%|█████████▏| 4.10G/4.48G [00:47<00:05, 71.0MB/s]\u001B[A\n\u001B[2m\u001B[36m(download_model pid=6964, ip=10.68.174.155)\u001B[0m \u001B[32m [repeated 319x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=6964, ip=10.68.174.155)\u001B[0m \rDownloading (…)l-00001-of-00002.bin:  54%|█████▍    | 5.42G/9.95G [00:50<01:28, 51.2MB/s]\u001B[A\u001B[A\u001B[32m [repeated 179x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=6964, ip=10.68.174.155)\u001B[0m \u001B[A\n\u001B[2m\u001B[36m(download_model pid=6992, ip=10.68.175.10)\u001B[0m \rDownloading (…)l-00002-of-00002.bin:  99%|█████████▉| 4.45G/4.48G [00:52<00:00, 70.8MB/s]\u001B[A\u001B[A\u001B[32m [repeated 131x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=6964, ip=10.68.174.155)\u001B[0m \rDownloading (…)l-00001-of-00002.bin:  56%|█████▌    | 5.59G/9.95G [00:52<01:01, 71.4MB/s]\u001B[A\u001B[A\rDownloading (…)l-00002-of-00002.bin: 100%|██████████| 4.48G/4.48G [00:52<00:00, 85.1MB/s]\n\u001B[2m\u001B[36m(download_model pid=7003, ip=10.68.166.174)\u001B[0m \u001B[32m [repeated 273x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=6992, ip=10.68.175.10)\u001B[0m \rDownloading (…)l-00001-of-00002.bin:  59%|█████▉    | 5.85G/9.95G [00:55<00:55, 73.9MB/s]\u001B[A\u001B[32m [repeated 135x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=6964, ip=10.68.174.155)\u001B[0m \rDownloading (…)l-00001-of-00002.bin:  59%|█████▉    | 5.87G/9.95G [00:56<00:39, 103MB/s] \u001B[A\u001B[A\n\u001B[2m\u001B[36m(download_model pid=6886, ip=10.68.178.177)\u001B[0m \rDownloading (…)l-00001-of-00002.bin:  60%|█████▉    | 5.92G/9.95G [00:56<00:38, 104MB/s] \u001B[A\n\u001B[2m\u001B[36m(download_model pid=6992, ip=10.68.175.10)\u001B[0m \rDownloading (…)l-00002-of-00002.bin: 100%|██████████| 4.48G/4.48G [00:52<00:00, 84.8MB/s]\u001B[32m [repeated 13x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=6886, ip=10.68.178.177)\u001B[0m \rDownloading (…)l-00001-of-00002.bin:  61%|██████    | 6.04G/9.95G [00:57<00:34, 112MB/s]\u001B[A\n\u001B[2m\u001B[36m(download_model pid=6992, ip=10.68.175.10)\u001B[0m \u001B[32m [repeated 193x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=7003, ip=10.68.166.174)\u001B[0m \rDownloading (…)l-00001-of-00002.bin:  65%|██████▌   | 6.50G/9.95G [01:00<00:24, 142MB/s]\u001B[A\u001B[A\u001B[32m [repeated 121x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=7003, ip=10.68.166.174)\u001B[0m \rDownloading (…)l-00001-of-00002.bin:  66%|██████▌   | 6.54G/9.95G [01:00<00:30, 110MB/s] \u001B[A\u001B[A\u001B[32m [repeated 3x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=6992, ip=10.68.175.10)\u001B[0m \rDownloading (…)l-00001-of-00002.bin:  66%|██████▌   | 6.59G/9.95G [01:01<00:22, 147MB/s]\u001B[A\n\u001B[2m\u001B[36m(download_model pid=6964, ip=10.68.174.155)\u001B[0m \u001B[32m [repeated 196x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=6992, ip=10.68.175.10)\u001B[0m \rDownloading (…)l-00001-of-00002.bin:  72%|███████▏  | 7.17G/9.95G [01:05<00:23, 118MB/s]\u001B[A\u001B[32m [repeated 129x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=7003, ip=10.68.166.174)\u001B[0m \rDownloading (…)l-00001-of-00002.bin:  72%|███████▏  | 7.20G/9.95G [01:05<00:27, 101MB/s] \u001B[A\u001B[A\u001B[32m [repeated 3x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=6964, ip=10.68.174.155)\u001B[0m \u001B[32m [repeated 202x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=6992, ip=10.68.175.10)\u001B[0m \rDownloading (…)l-00001-of-00002.bin:  80%|███████▉  | 7.94G/9.95G [01:10<00:13, 150MB/s]\u001B[A\u001B[32m [repeated 138x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=7003, ip=10.68.166.174)\u001B[0m \rDownloading (…)l-00001-of-00002.bin:  81%|████████  | 8.05G/9.95G [01:11<00:12, 152MB/s]\u001B[A\u001B[A\n\u001B[2m\u001B[36m(download_model pid=7003, ip=10.68.166.174)\u001B[0m \rDownloading (…)l-00001-of-00002.bin:  83%|████████▎ | 8.28G/9.95G [01:12<00:15, 106MB/s] \u001B[A\u001B[A\n\u001B[2m\u001B[36m(download_model pid=6886, ip=10.68.178.177)\u001B[0m \u001B[32m [repeated 192x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=6886, ip=10.68.178.177)\u001B[0m \rDownloading (…)l-00001-of-00002.bin:  85%|████████▌ | 8.48G/9.95G [01:14<00:13, 108MB/s] \u001B[A\n\u001B[2m\u001B[36m(download_model pid=6886, ip=10.68.178.177)\u001B[0m \rDownloading (…)l-00001-of-00002.bin:  87%|████████▋ | 8.66G/9.95G [01:15<00:11, 108MB/s]\u001B[A\u001B[32m [repeated 124x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=6964, ip=10.68.174.155)\u001B[0m \rDownloading (…)l-00001-of-00002.bin:  88%|████████▊ | 8.75G/9.95G [01:16<00:10, 118MB/s] \u001B[A\u001B[A\u001B[32m [repeated 4x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=7003, ip=10.68.166.174)\u001B[0m \rDownloading (…)l-00001-of-00002.bin:  91%|█████████▏| 9.08G/9.95G [01:18<00:04, 176MB/s]\u001B[A\u001B[A\n\u001B[2m\u001B[36m(download_model pid=7003, ip=10.68.166.174)\u001B[0m \rDownloading (…)l-00001-of-00002.bin:  92%|█████████▏| 9.14G/9.95G [01:19<00:07, 104MB/s] \u001B[A\u001B[A\n\u001B[2m\u001B[36m(download_model pid=6886, ip=10.68.178.177)\u001B[0m \rDownloading (…)l-00001-of-00002.bin:  92%|█████████▏| 9.15G/9.95G [01:19<00:07, 105MB/s] \u001B[A\n\u001B[2m\u001B[36m(download_model pid=6992, ip=10.68.175.10)\u001B[0m \u001B[32m [repeated 185x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=6992, ip=10.68.175.10)\u001B[0m \rDownloading (…)l-00001-of-00002.bin:  90%|████████▉ | 8.92G/9.95G [01:17<00:08, 116MB/s] \u001B[A\u001B[32m [repeated 5x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=6964, ip=10.68.174.155)\u001B[0m \rDownloading (…)l-00001-of-00002.bin:  91%|█████████ | 9.07G/9.95G [01:18<00:05, 168MB/s]\u001B[A\u001B[A\u001B[32m [repeated 68x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=6992, ip=10.68.175.10)\u001B[0m \u001B[A\n\u001B[2m\u001B[36m(download_model pid=6886, ip=10.68.178.177)\u001B[0m \rDownloading (…)l-00001-of-00002.bin:  98%|█████████▊| 9.76G/9.95G [01:23<00:01, 133MB/s]\u001B[A\u001B[32m [repeated 123x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=6964, ip=10.68.174.155)\u001B[0m \rDownloading (…)l-00001-of-00002.bin:  99%|█████████▉| 9.84G/9.95G [01:24<00:00, 126MB/s]\u001B[A\n\u001B[2m\u001B[36m(download_model pid=6992, ip=10.68.175.10)\u001B[0m \rDownloading (…)l-00001-of-00002.bin: 100%|█████████▉| 9.91G/9.95G [01:24<00:00, 133MB/s]\u001B[A\u001B[A\n\u001B[2m\u001B[36m(download_model pid=6886, ip=10.68.178.177)\u001B[0m \u001B[32m [repeated 199x across cluster]\u001B[0m\n\u001B[2m\u001B[36m(download_model pid=7003, ip=10.68.166.174)\u001B[0m \rDownloading (…)l-00001-of-00002.bin: 100%|█████████▉| 9.95G/9.95G [01:24<00:00, 136MB/s]\u001B[A\u001B[A\rDownloading (…)l-00001-of-00002.bin: 100%|██████████| 9.95G/9.95G [01:24<00:00, 117MB/s]\n\u001B[2m\u001B[36m(download_model pid=7003, ip=10.68.166.174)\u001B[0m \rFetching 12 files:  58%|█████▊    | 7/12 [01:27<01:09, 13.89s/it]\rFetching 12 files: 100%|██████████| 12/12 [01:27<00:00,  7.27s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def force_on_node(node_id: str, remote_func_or_actor_class):\n",
    "    scheduling_strategy = ray.util.scheduling_strategies.NodeAffinitySchedulingStrategy(\n",
    "        node_id=node_id, soft=False\n",
    "    )\n",
    "    options = {\"scheduling_strategy\": scheduling_strategy}\n",
    "    return remote_func_or_actor_class.options(**options)\n",
    "\n",
    "\n",
    "def run_on_every_node(remote_func_or_actor_class, **remote_kwargs):\n",
    "    refs = []\n",
    "    for node in ray.nodes():\n",
    "        if node[\"Alive\"] and node[\"Resources\"].get(\"GPU\", None):\n",
    "            refs.append(\n",
    "                force_on_node(node[\"NodeID\"], remote_func_or_actor_class).remote(\n",
    "                    **remote_kwargs\n",
    "                )\n",
    "            )\n",
    "    return ray.get(refs)\n",
    "\n",
    "\n",
    "@ray.remote(num_gpus=1)\n",
    "def download_model():\n",
    "    snapshot_download(pretrained_model_name_or_path,resume_download=True) \n",
    "  \n",
    "\n",
    "_ = run_on_every_node(download_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34c1e06b-5b50-4133-a7d8-f067713d4c77",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Loading the dataset <a name=\"load\"></a>\n",
    "\n",
    "We will be fine-tuning the model on the the Databricks crowd sourced dataset , it comprised of 15,000 lines of Question and Answer pairs . \n",
    "\n",
    "We will use [Ray Data](https://raw.githubusercontent.com/ray-project/ray/master/doc/source/ray-air/examples/data) for distributed preprocessing and data ingestion. We can easily convert the dataset obtained from Hugging Face Hub to Ray Data by using `ray.data.from_huggingface` \n",
    "\n",
    "Note ingestion from Delta,Parquet,CSV is also supported via Ray Data API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a908d442-7fdf-4887-86bc-f13de83ee654",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f695c84ca8a401092b88a7b2099f115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/501 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Splitting the data into test and train \n",
    "\n",
    "\n",
    "current_dataset  = load_dataset(\n",
    "                  dataset_path,\n",
    "                  streaming=True )\n",
    "# current_dataset = load_training_dataset()\n",
    "# current_dataset = current_dataset.train_test_split(seed=DEFAULT_SEED)\n",
    "\n",
    "# current_dataset['train'].select(list(range(0,1000))).save_to_disk(\"/local_disk0/train.hf\")\n",
    "# current_dataset['test'].select(list(range(0,1000))).save_to_disk('/local_disk0/test.hf')\n",
    "\n",
    "# # current_dataset['train'].save_to_disk(\"/local_disk0/train.hf\")\n",
    "# # current_dataset['test'].save_to_disk('/local_disk0/test.hf')\n",
    "# del current_dataset\n",
    "\n",
    "# # load the final data as ray data-set\n",
    "# train_dataset = ray.data.from_huggingface(load_from_disk('/local_disk0/train.hf'))\n",
    "# test_dataset = ray.data.from_huggingface(load_from_disk('/local_disk0/test.hf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99422661-72b0-4f82-b57e-e4a468b077b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset_type\n",
    "d_type_split = dataset_type.split(\":\")\n",
    "d_base_type = d_type_split[0]\n",
    "d_prompt_style = d_type_split[1] if len(d_type_split) > 1 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07923d8c-6856-4a99-be4d-2413acdc5d34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'alpaca'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_base_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1183d4d-e9a4-415c-96f5-bf471fab4332",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Preprocess the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fbc64c9-8eaf-46d3-8d30-f1bbaed9a837",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We will need to do some preprocessing. For that, we will define two [Ray AIR Preprocessors](https://raw.githubusercontent.com/ray-project/ray/master/doc/source/ray-air/examples/air-preprocessors) using the {class}`~ray.data.preprocessors.BatchMapper` API, allowing us to define functions that will be applied on batches of data.\n",
    "\n",
    "The `preprocess` function will call The `tokenize` function will take the lines and tokenize them using the 🤗 Tokenizer associated with the model, ensuring each entry has the same length (`max_length`) by padding and truncating. This is necessary for training.\n",
    "\n",
    "```{note}\n",
    "This preprocessing can be done in other ways. A common pattern is to tokenize first, and then split the obtained tokens into equally-sized blocks.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ea6452d-c49f-4a96-91fe-f306eac32990",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "from pathlib import Path\n",
    "from ray import tune\n",
    "from datasets import Dataset\n",
    "from ray.data.preprocessors import Chain, BatchMapper\n",
    "from ray.air.util.check_ingest import DummyTrainer\n",
    "from ray.air.config import ScalingConfig,RunConfig,CheckpointConfig\n",
    "\n",
    "\n",
    "from training.trainer import load_tokenizer,preprocess_batch,\\\n",
    "                             DataCollatorForCompletionOnlyLM,get_model_tokenizer\n",
    "\n",
    "def preprocess(batch):\n",
    "  tokenizer = load_tokenizer(pretrained_model_name_or_path)\n",
    "  seed=DEFAULT_SEED\n",
    "  dataset = Dataset.from_pandas(batch)\n",
    "  _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n",
    "  dataset = dataset.map(\n",
    "      _preprocessing_function,\n",
    "      batched=True,\n",
    "      remove_columns=[\"instruction\", \"context\", \"response\", \"text\", \"category\"],\n",
    "  )\n",
    "\n",
    "  # Make sure we don't have any truncated records, as this would mean the end keyword is missing.\n",
    "  dataset = dataset.filter(lambda rec: len(rec[\"input_ids\"]) < max_length)\n",
    "  dataset = dataset.shuffle(seed=seed)\n",
    "  return dataset.to_pandas()\n",
    "\n",
    "\n",
    "preprocessor = Chain(\n",
    "    BatchMapper(preprocess, batch_format=\"pandas\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "265069af-bfff-4283-9b6d-a03b57959844",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Fine-tuning the model with Ray AIR <a name=\"train\"></a>\n",
    "\n",
    "We can now configure Ray AIR's {class}`~ray.train.huggingface.huggingface_trainer.HuggingFaceTrainer` to perform distributed fine-tuning of the model. In order to do that, we specify a `trainer_init_per_worker` function, which creates a 🤗 Transformers `Trainer` that will be distributed by Ray using Distributed Data Parallelism (using PyTorch Distributed backend internally). This means that each worker will have its own copy of the model, but operate on different data, At the end of each step, all the workers will sync gradients.\n",
    "\n",
    "Because pythia-12b is a relatively large model, it may not be possible to fit it on smaller GPU types (<=16 GB GRAM). To deal with that issue, we can use [DeepSpeed](https://github.com/microsoft/DeepSpeed), a library to optimize the training process and allow us to (among other things) offload and partition optimizer and parameter states, reducing GRAM usage. Furthermore, DeepSpeed ZeRO Stage 3 allows us to load large models without running out of memory.\n",
    "\n",
    "🤗 Transformers and Ray AIR's integration ({class}`~ray.train.huggingface.huggingface_trainer.HuggingFaceTrainer`) allow you to easily configure and use DDP and DeepSpeed. All you need to do is specify the DeepSpeed configuration in the [`TrainingArguments`](https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments) object.\n",
    "\n",
    "```{tip}\n",
    "There are many DeepSpeed settings that allow you to trade-off speed for memory usage. The settings used below are tailored to the cluster setup used (16 g4dn.4xlarge nodes) and per device batch size of 16. Some things to keep in mind:\n",
    "- If your GPUs support bfloat16, use that instead of float16 mixed precision to get better performance and prevent overflows. Replace `fp16=True` with `bf16=True` in `TrainingArguments`.\n",
    "- If you are running out of GRAM: try reducing batch size (defined in the cell below the next one), set `\"overlap_comm\": False` in DeepSpeed config.\n",
    "- If you are running out of RAM, add more nodes to your cluster, use nodes with more RAM, set `\"pin_memory\": False` in the DeepSpeed config, reduce the batch size, and remove `\"offload_param\"` from the DeepSpeed config.\n",
    "\n",
    "For more information on DeepSpeed configuration, refer to [Hugging Face documentation](https://huggingface.co/docs/transformers/main_classes/deepspeed) and [DeepSpeed documentation](https://www.deepspeed.ai/docs/config-json/).\n",
    "\n",
    "Additionally, if you prefer a lower-level API, the logic below can be expressed as an [Accelerate training loop](https://github.com/huggingface/accelerate/blob/main/examples/by_feature/deepspeed_with_config_support.py) distributed by a Ray AIR {class}`~ray.train.torch.torch_trainer.TorchTrainer`.\n",
    "```\n",
    "\n",
    "#### Training speed\n",
    "\n",
    "As we are using data parallelism, each worker operates on its own shard of the data. The batch size set in `TrainingArguments` is the **per device batch size** (per worker batch size). By changing the number of workers, we can change the **effective batch size** and thus the time needed for training to complete. The effective batch size is then calculated as `per device batch size * number of workers * number of gradient accumulation steps`. As we add more workers, the effective batch size rises and thus we need less time to complete a full epoch. While the speedup is not exactly linear due to extra communication overheads, in many cases it can be close to linear.\n",
    "\n",
    "The preprocessed dataset has ~15000 examples. We have set per device batch size to 10.\n",
    "\n",
    "* With 4 g5.24xlarge nodes, the effective batch size was 160, which equals to 85 steps per epoch. two epoch took 2.27 hours (including initialization and saving time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61c6f272-2cb8-458e-a66b-2e90cb6d4286",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def trainer_init_per_worker(train_dataset, eval_dataset=None, **config):\n",
    "\n",
    "    set_seed(seed)\n",
    "\n",
    "    # Use the actual number of CPUs assigned by Ray\n",
    "    os.environ[\"OMP_NUM_THREADS\"] = str(\n",
    "        session.get_trial_resources().bundles[-1].get(\"CPU\", 1)\n",
    "    )\n",
    "    # Enable tf32 for better performance\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "    # Get config details\n",
    "\n",
    "    epochs = config.get(\"epochs\")\n",
    "    lr = config.get(\"lr\")\n",
    "    per_device_train_batch_size = config.get(\"per_device_train_batch_size\")\n",
    "    per_device_eval_batch_size = config.get(\"per_device_eval_batch_size\")\n",
    "    logging_steps = config.get(\"logging_steps\")\n",
    "    save_strategy= config.get(\"save_strategy\")\n",
    "    evaluation_strategy = config.get(\"evaluation_strategy\")\n",
    "    save_steps = config.get(\"save_steps\")\n",
    "    eval_steps = config.get(\"eval_steps\") \n",
    "    warmup_steps = config.get(\"warmup_steps\")\n",
    "    disable_tqdm=config.get(\"disable_tqdm\")\n",
    "    remove_unused_columns=config.get(\"remove_unused_columns\")\n",
    "    deepspeed=config.get(\"deepspeed\", \"configs/ds_z3_bf16_config.json\")\n",
    "\n",
    "    with open('/tmp'+'/deepspeed.json', 'w') as f:\n",
    "      json.dump(deepspeed, f)\n",
    "\n",
    "    print(\"Preparing training arguments\")\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=local_output_dir,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        fp16=False, # change to true if using v100\n",
    "        bf16=True,# chenge to false if using v100\n",
    "        learning_rate=lr,\n",
    "        num_train_epochs=epochs,\n",
    "        deepspeed='/tmp'+'/deepspeed.json',\n",
    "        gradient_checkpointing=gradient_checkpointing,\n",
    "        logging_strategy=evaluation_strategy,\n",
    "        logging_steps=logging_steps,\n",
    "        evaluation_strategy=evaluation_strategy,\n",
    "        eval_steps=eval_steps,\n",
    "        save_strategy=save_strategy,\n",
    "        save_steps=save_steps,\n",
    "        load_best_model_at_end=False,\n",
    "        disable_tqdm=disable_tqdm,\n",
    "        remove_unused_columns=remove_unused_columns,\n",
    "        warmup_steps=warmup_steps)\n",
    "\n",
    "    print(\"Loading model\")\n",
    "\n",
    "    model, tokenizer = get_model_tokenizer(\n",
    "        pretrained_model_name_or_path=pretrained_model_name_or_path, gradient_checkpointing=gradient_checkpointing\n",
    "    )\n",
    "\n",
    "    print(\"Model loaded\")\n",
    "    print(\"Train data size: %d\", len(train_dataset))\n",
    "    print(\"Test data size: %d\", len(eval_dataset))\n",
    "\n",
    "    data_collator = DataCollatorForCompletionOnlyLM(\n",
    "        tokenizer=tokenizer, mlm=False, return_tensors=\"pt\", pad_to_multiple_of=8\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2fccce7-b15e-4029-850e-a7df215b15d7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "With our `trainer_init_per_worker` complete, we can now instantiate the {class}`~ray.train.huggingface.huggingface_trainer.HuggingFaceTrainer`. Aside from the function, we set the `scaling_config`, controlling the amount of workers and resources used, and the `datasets` we will use for training and evaluation.\n",
    "\n",
    "We pass the preprocessors we have defined earlier as an argument, wrapped in a {class}`~ray.data.preprocessors.chain.Chain`. The preprocessor will be included with the returned {class}`~ray.air.checkpoint.Checkpoint`, meaning it will also be applied during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c534fa7-d756-4a4d-b298-5d17e5373090",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get or create experiment\n",
    "get_or_create_experiment(experiment_location)\n",
    "\n",
    "\n",
    "sync_config = tune.SyncConfig(\n",
    "  syncer=None\n",
    "    # upload_dir=\"s3://one-env-eu-west-1/users/puneet.jain@databricks.com/\",  # requires AWS credential\n",
    "    )\n",
    "#Create tags to log with mlflow\n",
    "tags = dict(\n",
    "  local_dir = f\"/dbfs/{username}/dolly_train/job/\",\n",
    "  base_model_dir = pretrained_model_name_or_path,\n",
    "  n_gpus = str(num_workers),\n",
    "  num_cpu_cores_per_worker = str(num_cpu_cores_per_worker),\n",
    "  num_gpu_per_worker = str(num_gpu_per_worker),  \n",
    "  max_length = str(max_length),\n",
    "  username = username )\n",
    "\n",
    "root_path = os.getcwd()\n",
    "deepspeed_config = os.path.join(root_path, \"config/ds_z3_bf16_config.json\")\n",
    "\n",
    "with open(deepspeed_config) as json_data:\n",
    "    deepspeed_config = json.load(json_data)\n",
    "\n",
    "trainer = HuggingFaceTrainer(\n",
    "    trainer_init_per_worker=trainer_init_per_worker,\n",
    "    trainer_init_config={\n",
    "        \"deepspeed\": deepspeed_config, \n",
    "        \"lr\" : 1e-6, # per device\n",
    "        \"per_device_train_batch_size\" : 10,\n",
    "        \"per_device_eval_batch_size\" : 10,\n",
    "        \"save_strategy\" : \"no\",\n",
    "        \"evaluation_strategy\" : \"steps\",\n",
    "        \"logging_steps\" : 50,\n",
    "        \"save_steps\" : 200,\n",
    "        \"eval_steps\" : 50,\n",
    "        \"warmup_steps\" : 25,\n",
    "        \"disable_tqdm\" : True,\n",
    "        \"remove_unused_columns\" :False,\n",
    "        \"epochs\": 3},\n",
    "    scaling_config=ScalingConfig(\n",
    "        num_workers=16,\n",
    "        use_gpu=use_gpu,\n",
    "        resources_per_worker={\"GPU\": 1, \n",
    "                              \"CPU\": 22}), # should be total cores in node /total gpu's in node -2\n",
    "    run_config = RunConfig(\n",
    "                local_dir =  f\"/dbfs/{username}/dolly_train/job/\",\n",
    "                callbacks=[MLflowLoggerCallback(experiment_name=experiment_location,\n",
    "                                                tags = tags,\n",
    "                                                save_artifact=False)],\n",
    "                sync_config=sync_config,\n",
    "                checkpoint_config = CheckpointConfig(num_to_keep = 1, \n",
    "                                                     checkpoint_score_attribute = 'eval_loss',\n",
    "                                                     checkpoint_score_order = 'min') \n",
    "    ),\n",
    "    datasets={\"train\": train_dataset ,\n",
    "              \"evaluation\" : test_dataset},\n",
    "    preprocessor=preprocessor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c07dae1-e44c-4632-a54b-64235c23fdff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Finally, we call the `~ray.train.huggingface.huggingface_trainer.HuggingFaceTrainer.fit` method to start training with Ray AIR. We will save the `~ray.air.Result` object to a variable so we can access metrics and checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb43fc69-73d9-4c41-b1cc-db87878653a8",
     "showTitle": false,
     "title": "glltddukjcuvujehr"
    }
   },
   "outputs": [],
   "source": [
    "results = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7772e6a0-e662-4e9f-b572-4e8b7808f5bf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Fetch the best model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10c762e8-a48c-4947-8657-64646a99e65f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "You can use the returned {class}`~ray.air.Result` object to access metrics and the Ray AIR {class}`~ray.air.checkpoint.Checkpoint` associated with the last iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a36c348-9a0a-45fe-890c-c6fa26be9448",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "checkpoint = results.checkpoint\n",
    "checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c859c5bd-c88c-4f02-80fa-a8bee912da92",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Generate text from prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d20b6a4e-d712-4cbf-ae87-aaaeffd3f8f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import ray.data\n",
    "import pandas as pd\n",
    "from training.generate import PredictCallable\n",
    "\n",
    "instructions = [\n",
    "    \"Write a love letter to Edgar Allan Poe.\",\n",
    "    \"Write a tweet announcing Dolly, a large language model from Databricks.\",\n",
    "    \"I'm selling my Nikon D-750, write a short blurb for my ad.\",\n",
    "    \"Explain to me the difference between nuclear fission and fusion.\",\n",
    "    \"Give me a list of 5 science fiction books I should read next.\"]\n",
    "\n",
    "ds = ray.data.from_pandas(pd.DataFrame(pd.Series(instructions),columns=[\"prompt\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81487f99-62e2-46c2-b684-02dbcd7a8914",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "preds = (\n",
    "    ds\n",
    "    .repartition(5)\n",
    "    .map_batches(\n",
    "        PredictCallable,\n",
    "        batch_size=1,\n",
    "        fn_constructor_kwargs=dict(checkpoint=checkpoint.uri.split('file://')[1],\n",
    "                                   torch_dtype = \"bfloat16\" ),#change to float16 when using V100\n",
    "        batch_format=\"pandas\",\n",
    "        compute=ray.data.ActorPoolStrategy(),\n",
    "        num_gpus=4,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7a44acf-af84-48e0-a144-2d636c2a2af9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "preds.take_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0fe288bb-cb68-4f33-8d36-9caf75c2d322",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "falcon-7b-fine-tune-multi-gpu",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
